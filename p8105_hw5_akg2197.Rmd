---
title: "p8105_hw5_akg2197"
author: "Annie Gorman"
date: "2024-11-12"
output: github_document
---

## Loading libraries 

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1 

Writing a function that, for a fixed group size, randomly draws “birthdays” for each person; checks whether there are duplicate birthdays in the group; and returns TRUE or FALSE based on the result:

```{r}
bday_sim = function(n) {
  bdays = sample(1:365, size = n, replace = TRUE)
  duplicate = length(unique(bdays)) < n
  return(duplicate)
}

bday_sim(10)
```

We will run this function 10000 times for each group size between 2 and 50. Here, we'll also generate a plot looking at probability that at least two people share a birthday as a function of group size. 

```{r}
sim_res = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |> 
  mutate(res = map_lgl(n, bday_sim)) |> 
  group_by(n) |> 
  summarize(prob = mean(res))

sim_res |> 
  ggplot(aes(x = n, y = prob )) + 
  geom_line()
```

From this plot, we can see that as group size increases, the probability that at least two people share a birthday in the room also increases.  

```{r}
sim_regression = function(n) {
  
  sim_data = 
    tibble(
      x = rnorm(n, mean = 1, sd = 1),
      y = 2 + 3 * x + rnorm(n, 0, 1)
    )

  lm_fit = lm(y ~ x, data = sim_data)

  out_df = 
    tibble(
      beta0_hat = coef(lm_fit)[1],
      beta1_hat = coef(lm_fit)[2]
    )
  
  return(out_df)
}

sim_res = 
  expand_grid(
    sample_size = c(30, 60), 
    iter = 1:1000
  ) |> 
  mutate(lm_res = map(sample_size, sim_regression)) |> 
  unnest(lm_res)

sim_res |> 
  mutate(sample_size = str_c("n = ", sample_size)) |> 
  ggplot(aes(x = sample_size, y = beta1_hat)) + 
  geom_boxplot()

```

We can filter here for beta when sample size is equal to 30: 

```{r}
sim_res |> 
  filter(sample_size == 30) |> 
  ggplot(aes(x = beta0_hat, y = beta1_hat)) +
  geom_point()
```

# Problem 2

### Function and map
```{r}
sim_function <- function(n = 30, mu = 0, sigma = 5) {
  sim_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma)
  )
  test = t.test(x ~ 1, data = sim_data)
  broom::tidy(test)
}
```

```{r}
sim_results = tibble(mu = 0:6) |>
  mutate(
    output_df = map(.x = mu, ~rerun(5000, sim_function(n = 30, mu=.x))) ,
    value_df = map(output_df, bind_rows)
  )
```

```{r}
sim_results = sim_results |>
  unnest(value_df) |>
  select(estimate, p.value, mu) |>
  mutate(significant_results = as.numeric(p.value < 0.05))
```

### Let's make some plots using the above function outputs

#### Plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis:

```{r}
sim_results |>
  group_by(mu) |>
  summarize(power = mean(significant_results)) |>
  ggplot(aes(x = mu, y = power)) + 
  geom_path() 
```

From this plot, we can see that as effect size increases, so does power. Although this relationship is not linear, we do observe a direct relationship between effect size and power. Effect size describes the difference between the true statistics and the null statistics.

#### Plot showing the average estimate of mu on the y axis and the true value of mu on the x axis

```{r}
sim_results |>
  group_by(mu) |>
  summarize(estimatedtrue_mu = mean(estimate)) |>
  ggplot(aes(x = mu, y = estimatedtrue_mu)) + 
  geom_point() + 
  geom_path() 
```

#### Avg estimate of mu where y = null was rejected and x = true value of mu 
```{r}
sim_results |>
  filter(significant_results == 1) |>
  group_by(mu) |>
  summarize(estimatedtrue_mu = mean(estimate)) |>
  ggplot(aes(x = mu, y = estimatedtrue_mu)) + 
  geom_point() + 
  geom_path() 
```

When mu is equal to 1, 2, or 3, the the sample average of mu across tests for which the null is rejected is NOT equal to the true value of mu. For mu = 4, 5, or 6, the average of mu is approximately equal to the true value of mu. This is because higher values lead to rejection of the mu (significant result). However, when we have smaller values like 1, 2, or 3, a smaller proportion of the sample gets rejected, and many of those values will have sample values larger than the true mu. This would result in unequal sample averages to the true mu, especially sample averages that are higher than the true mu.  

# Problem 3 

#### Reading in raw data and making city state variable
```{r}
homicide_df <- read_csv("homicide-data.csv") |>
    mutate(city_state = map2(city, state, ~ paste(.x, .y, sep = ","))) 
```
This raw data includes variables of homicides, including ID number sequence, date homicide was reported, victim race, age, last name, first name, and sex, as well as the city, state, latitude, and longitude where the murder was committed. Disposition (details of arrest) is also included. I added the city, state variable here as well as a variable for the number of unsolved homicides and total homicides in each city and state. 

#### City/state variable and summarizing homicides 
I already created my city, state variable. Now, I will be summarizing within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”). To do this, I will create a new df with counts "closed without arrest" and "open/no arrest" (unsolved cases) as well as total number of homicides. 

```{r}
unsolved_counts <- homicide_df |>
  group_by(city_state) |>
  summarise(total_count = n(),
            unsolved_count = sum(disposition %in% c("Closed without arrest", "Open/No arrest")), .groups = "drop")
```

#### Estimating the proportion of homicides that are unsolved in Baltimore, MD

```{r}
baltimore_results <- 
  prop.test(
    x = unsolved_counts |>
      filter(city_state == "Baltimore,MD") |>
      pull(unsolved_count), 
    n = unsolved_counts |>
      filter(city_state == "Baltimore,MD") |>
      pull(total_count))

broom::tidy(baltimore_results) |>
  select(estimate, conf.low, conf.high)
```
#### Estimating the proportion of homicides that are unsolved in all city states 

Now I will write a function to get these values for each of the cities in the dataset:

```{r}
city_function <- function(cityname) { 
  city_results <-
    prop.test(
    x = unsolved_counts |>
      filter(city_state == cityname) |>
      pull(unsolved_count), 
    n = unsolved_counts |>
      filter(city_state == cityname) |>
      pull(total_count))

broom::tidy(city_results) |>
  mutate(city_state = cityname) |>
  select(city_state, estimate, conf.low, conf.high)
}
```

Mapping this function so we can apply it to all cities and run the proportion test:

```{r}
city_results <- unsolved_counts |>
  pull(city_state) |>  
  unique() |>          
  purrr::map(~city_function(.)) |>  
  bind_rows() 

city_results
```

#### Plotting estimates and CIs for proportion of unsolved homicides in each city

Now that we've applied our function to all the cities, we can create a plot that shows the estimates and CIs for each city

```{r}
city_results |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate, color = city_state)) +
  geom_point(size = 3) + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2, color = "black") + 
  coord_flip() +
  labs(
    title = "Proportion Estimates of Unsolved Homicides by US City",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

